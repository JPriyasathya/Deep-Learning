# -*- coding: utf-8 -*-
"""DL MINI_PRO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vgfdRqG9iXQwcBvkqTpiek0Obhta0zpc

**NAMED ENTITY RECOGNITION**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import ast
import tensorflow as tf
import sklearn
from sklearn import model_selection

data = pd.read_csv('/content/drive/MyDrive/ner.csv',encoding='latin1')
data.head()

data.dropna(inplace=True)
print("Number of rows : ",data.shape[0]," and the number of columns : ",data.shape[1])

for i in range(len(data)):
   pos = ast.literal_eval(data['POS'][i])
   tags = ast.literal_eval(data['Tag'][i])
   data['POS'][i] = [str(word) for word in pos]
   data['Tag'][i] = [str(word.upper()) for word in tags]

print (data)

data["Tag"][0]

df_final = data[['Sentence','Tag']]

df_train,df_test = model_selection.train_test_split(df_final,test_size=0.2,random_state=42)
len(df_train),len(df_test)

from keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

from keras.layers import Dense,Input,Embedding,Flatten
from keras.models import Model
from keras.losses import SparseCategoricalCrossentropy

train_targets = list(df_train.Tag.values)
test_targets = list(df_test.Tag.values)

tokenizer = Tokenizer(lower=False,oov_token="UNK")
tokenizer.fit_on_texts(df_train['Sentence'])

train_inputs = tokenizer.texts_to_sequences(df_train['Sentence'])
test_inputs = tokenizer.texts_to_sequences(df_test['Sentence'])

word2idx = tokenizer.word_index
V = len(word2idx)
print("Found %s unique tokens "%V)

train_tags = set([val for sublist in train_targets for val in sublist])
test_tags = set([val for sublist in test_targets for val in sublist])

print("Unique NER tags in train set: ",train_tags)
print("Unique NER tags in test set: ",test_tags)

tag_tokenizer = Tokenizer()
tag_tokenizer.fit_on_texts(train_targets)
train_tgt_int = tag_tokenizer.texts_to_sequences(train_targets)
test_tgt_int = tag_tokenizer.texts_to_sequences(test_targets)

max_length_train = max(len(sent) for sent in train_inputs)
max_length_test = max(len(sent) for sent in test_inputs)
max_length = max(max_length_train,max_length_test)

train_inputs_final = pad_sequences(train_inputs,maxlen=max_length,padding="post")
print("Shape of train inputs: ",train_inputs_final.shape)

test_inputs_final = pad_sequences(test_inputs,maxlen=max_length,padding="post")
print("Shape of test inputs: ",test_inputs_final.shape)

train_targets_final = pad_sequences(train_tgt_int,maxlen=max_length,padding="post")
print("Shape of train targets: ",train_targets_final.shape)

test_targets_final = pad_sequences(test_tgt_int,maxlen=max_length,padding="post")
print("Shape of test targets: ",test_targets_final.shape)

K = len(tag_tokenizer.word_index)  +1
K
vector_size = 16

"""RNN"""

from keras.layers import SimpleRNN

i = Input(shape=(max_length,))
x = Embedding(input_dim=V + 1, output_dim=vector_size, mask_zero=True)(i)
x = SimpleRNN(32, return_sequences=True)(x)
x = Dense(K)(x)

model_rnn = Model(i, x)
model_rnn.compile(optimizer="adam", loss=SparseCategoricalCrossentropy(from_logits=True), metrics=["accuracy"])
model_rnn.summary()

# Train and evaluate RNN model
model_rnn.fit(train_inputs_final, train_targets_final, epochs=10, validation_data=(test_inputs_final, test_targets_final))

"""MLP"""

from keras.layers import TimeDistributed

i = Input(shape=(max_length,))
x = Embedding(input_dim=V + 1, output_dim=vector_size, mask_zero=True)(i)
x = TimeDistributed(Dense(128, activation='relu'))(x)
x = TimeDistributed(Dense(64, activation='relu'))(x)
x = TimeDistributed(Dense(K))(x)

model_mlp = Model(i, x)
model_mlp.compile(optimizer="adam", loss=SparseCategoricalCrossentropy(from_logits=True), metrics=["accuracy"])
model_mlp.summary()

# Train and evaluate MLP model
model_mlp.fit(train_inputs_final, train_targets_final, epochs=10, validation_data=(test_inputs_final, test_targets_final))

"""CNN"""

from keras.layers import Conv1D, TimeDistributed

i = Input(shape=(max_length,))
x = Embedding(input_dim=V + 1, output_dim=vector_size, mask_zero=True)(i)
x = Conv1D(32, 3, activation='relu', padding='same')(x)
x = Conv1D(32, 3, activation='relu', padding='same')(x)
x = TimeDistributed(Dense(32, activation='relu'))(x)
x = TimeDistributed(Dense(K))(x)

model_cnn = Model(i, x)
model_cnn.compile(optimizer="adam", loss=SparseCategoricalCrossentropy(from_logits=True), metrics=["accuracy"])
model_cnn.summary()

# Train and evaluate CNN model
model_cnn.fit(train_inputs_final, train_targets_final, epochs=10, validation_data=(test_inputs_final, test_targets_final))

"""LSTM"""

from keras.layers import Bidirectional, LSTM

i = Input(shape=(max_length,))
x = Embedding(input_dim=V + 1, output_dim=vector_size, mask_zero=True)(i)
x = Bidirectional(LSTM(32, return_sequences=True))(x)
x = Dense(K)(x)

model_lstm = Model(i, x)
model_lstm.compile(optimizer="adam", loss=SparseCategoricalCrossentropy(from_logits=True), metrics=["accuracy"])
model_lstm.summary()

# Train and evaluate LSTM model
model_lstm.fit(train_inputs_final, train_targets_final, epochs=10, validation_data=(test_inputs_final, test_targets_final))

from keras.layers import GRU

i = Input(shape=(max_length,))
x = Embedding(input_dim=V + 1, output_dim=vector_size, mask_zero=True)(i)
x = GRU(32, return_sequences=True)(x)
x = Dense(K)(x)

model_gru = Model(i, x)
model_gru.compile(optimizer="adam", loss=SparseCategoricalCrossentropy(from_logits=True), metrics=["accuracy"])
model_gru.summary()

model_gru.fit(train_inputs_final, train_targets_final, epochs=10, validation_data=(test_inputs_final, test_targets_final))

"""**EVALUATION ON SAMPLE SENTENCE ON EACH ALGORITHM**"""

sentence = "Polish Prime Minister Jaroslaw Kaczynski has voiced support for the deployment of 10 U.S. missile interceptors in Poland and guidance technology in the Czech Republic ."
predictions = model_rnn.predict(pad_sequences(tokenizer.texts_to_sequences([sentence]), maxlen=max_length, padding="post"))
prediction_ner = np.argmax(predictions, axis=-1)
print("RNN Predictions:", prediction_ner)

sentence = "Polish Prime Minister Jaroslaw Kaczynski has voiced support for the deployment of 10 U.S. missile interceptors in Poland and guidance technology in the Czech Republic."
predictions = model_mlp.predict(pad_sequences(tokenizer.texts_to_sequences([sentence]), maxlen=max_length, padding="post"))
prediction_ner = np.argmax(predictions, axis=-1)
print("MLP Predictions:", prediction_ner)

sentence = "Polish Prime Minister Jaroslaw Kaczynski has voiced support for the deployment of 10 U.S. missile interceptors in Poland and guidance technology in the Czech Republic."
predictions = model_cnn.predict(pad_sequences(tokenizer.texts_to_sequences([sentence]), maxlen=max_length, padding="post"))
prediction_ner = np.argmax(predictions, axis=-1)
print("CNN Predictions:", prediction_ner)

sentence = "Polish Prime Minister Jaroslaw Kaczynski has voiced support for the deployment of 10 U.S. missile interceptors in Poland and guidance technology in the Czech Republic ."
predictions = model_lstm.predict(pad_sequences(tokenizer.texts_to_sequences([sentence]), maxlen=max_length, padding="post"))
prediction_ner = np.argmax(predictions, axis=-1)
print("LSTM Predictions:", prediction_ner)

import matplotlib.pyplot as plt

final_metrics = {
    'RNN': {'train_accuracy': 0.9276, 'val_accuracy': 0.8955, 'train_loss': 0.2305, 'val_loss': 0.3959},
    'MLP': {'train_accuracy': 0.8820, 'val_accuracy': 0.8734, 'train_loss': 0.4028, 'val_loss': 0.4869},
    'CNN': {'train_accuracy': 0.9733, 'val_accuracy': 0.9624, 'train_loss': 0.0859, 'val_loss': 0.1374},
    'LSTM': {'train_accuracy': 0.9436, 'val_accuracy': 0.9064, 'train_loss': 0.1669, 'val_loss': 0.3340},
    'GRU': {'train_accuracy': 0.9290, 'val_accuracy': 0.9005, 'train_loss': 0.2227, 'val_loss': 0.3583},
}

# Extract the data for plotting
models = list(final_metrics.keys())
train_accuracies = [final_metrics[model]['train_accuracy'] for model in models]
val_accuracies = [final_metrics[model]['val_accuracy'] for model in models]
train_losses = [final_metrics[model]['train_loss'] for model in models]
val_losses = [final_metrics[model]['val_loss'] for model in models]

# Plot accuracies
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.plot(models, train_accuracies, label='Train Accuracy', alpha=0.7)
plt.plot(models, val_accuracies, label='Validation Accuracy', alpha=0.7)
plt.title('Accuracies')
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.legend()

# Plot losses
plt.subplot(1, 2, 2)
plt.plot(models, train_losses, label='Train Loss', alpha=0.7)
plt.plot(models, val_losses, label='Validation Loss', alpha=0.7)
plt.title('Losses')
plt.xlabel('Models')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

